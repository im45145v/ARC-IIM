# Alumni Management System - Environment Configuration
# Copy this file to .env and fill in your actual values

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
# PostgreSQL database connection settings
DB_HOST=localhost
DB_PORT=5432
DB_NAME=alumni_db
DB_USER=postgres
DB_PASSWORD=your_secure_password_here

# ============================================================================
# LINKEDIN ACCOUNT CONFIGURATION (Multi-Account Support)
# ============================================================================
# Configure multiple LinkedIn accounts for rate limiting and load distribution
# The system will automatically detect and load all numbered accounts (1, 2, 3, ...)
# until it encounters a missing number in the sequence.
#
# IMPORTANT: Use dedicated LinkedIn accounts for scraping, NOT personal accounts

# ============================================================================
# OPTION 1: Cookie-Based Authentication (RECOMMENDED)
# ============================================================================
# More secure and supports 2FA! Export cookies using:
#   python scripts/export_linkedin_cookies.py
#
# Then set the path to your cookies file:
LINKEDIN_COOKIES_FILE_1=cookies/linkedin_cookies_1.json
LINKEDIN_COOKIES_FILE_2=cookies/linkedin_cookies_2.json
LINKEDIN_COOKIES_FILE_3=cookies/linkedin_cookies_3.json

# ============================================================================
# OPTION 2: Credential-Based Authentication (Legacy)
# ============================================================================
# Less secure, requires disabling 2FA, but simpler to set up
# If cookies are provided above, these will be used as fallback

# Account 1 (Required if not using cookies)
LINKEDIN_EMAIL_1=scraper1@example.com
LINKEDIN_PASSWORD_1=password123

# Account 2 (Optional but recommended)
LINKEDIN_EMAIL_2=scraper2@example.com
LINKEDIN_PASSWORD_2=password456

# Account 3 (Optional)
LINKEDIN_EMAIL_3=scraper3@example.com
LINKEDIN_PASSWORD_3=password789

# Add more accounts as needed (4, 5, 6, ...)
# LINKEDIN_EMAIL_4=scraper4@example.com
# LINKEDIN_PASSWORD_4=password012

# ============================================================================
# SCRAPER CONFIGURATION
# ============================================================================
# Rate limiting and behavior settings

# Maximum number of profiles each account can scrape per day
# Recommended: 50-80 to avoid LinkedIn rate limits
# Default: 80
SCRAPER_DAILY_LIMIT_PER_ACCOUNT=80

# Delay between scraping requests (in seconds)
# Random delay will be chosen between min and max
# Recommended: 3-8 seconds to simulate human behavior
# Default: min=3, max=8
SCRAPER_MIN_DELAY=3
SCRAPER_MAX_DELAY=8

# Maximum number of retry attempts for failed scrapes
# Default: 3
SCRAPER_MAX_RETRIES=3

# Browser automation settings
# Run browser in headless mode (no visible window)
# Default: true
SCRAPER_HEADLESS=true

# Milliseconds delay between browser actions (simulates human speed)
# Default: 100
SCRAPER_SLOW_MO=100

# ============================================================================
# BACKBLAZE B2 STORAGE CONFIGURATION
# ============================================================================
# Cloud storage for LinkedIn profile PDFs
# Sign up at: https://www.backblaze.com/b2/cloud-storage.html

# B2 Application Key ID (found in B2 dashboard)
B2_APPLICATION_KEY_ID=your_b2_key_id_here

# B2 Application Key (secret key from B2 dashboard)
B2_APPLICATION_KEY=your_b2_application_key_here

# B2 Bucket Name (create bucket in B2 dashboard first)
B2_BUCKET_NAME=alumni-linkedin-profiles

# ============================================================================
# OPTIONAL CONFIGURATION
# ============================================================================

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# Default: INFO
# LOG_LEVEL=INFO

# Streamlit server configuration (if needed)
# STREAMLIT_SERVER_PORT=8501
# STREAMLIT_SERVER_ADDRESS=localhost

# ============================================================================
# GITHUB ACTIONS CONFIGURATION
# ============================================================================
# When running in GitHub Actions, these are set as repository secrets
# Do NOT set these in your local .env file
# Configure them in: Repository Settings → Secrets and variables → Actions
#
# Required secrets:
# - DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD
# - LINKEDIN_EMAIL_1, LINKEDIN_PASSWORD_1 (and additional accounts)
# - B2_APPLICATION_KEY_ID, B2_APPLICATION_KEY, B2_BUCKET_NAME
# - SCRAPER_DAILY_LIMIT_PER_ACCOUNT (optional)
# - SCRAPER_MIN_DELAY, SCRAPER_MAX_DELAY (optional)
# - SCRAPER_MAX_RETRIES (optional)

# ============================================================================
# SECURITY NOTES
# ============================================================================
# 1. NEVER commit this file with actual credentials to version control
# 2. Keep .env in .gitignore (it should already be there)
# 3. Use strong, unique passwords for all services
# 4. Rotate credentials periodically
# 5. Use dedicated LinkedIn accounts, not personal ones
# 6. Be aware of LinkedIn's Terms of Service regarding scraping
# 7. Monitor account usage to avoid suspension

# ============================================================================
# QUICK START CHECKLIST
# ============================================================================
# [ ] Copy this file to .env
# [ ] Set up PostgreSQL database (use docker-compose for local testing)
# [ ] Create 1-3 dedicated LinkedIn accounts
# [ ] Fill in LinkedIn credentials (LINKEDIN_EMAIL_1, LINKEDIN_PASSWORD_1, etc.)
# [ ] Create Backblaze B2 account and bucket
# [ ] Fill in B2 credentials (B2_APPLICATION_KEY_ID, B2_APPLICATION_KEY, B2_BUCKET_NAME)
# [ ] Adjust scraper settings if needed (delays, limits)
# [ ] Test connection: python -c "from alumni_system.database.init_db import init_database; init_database()"
# [ ] Run application: streamlit run alumni_system/frontend/app.py